---
title: "Bootstrapping"
output: github_document
---

```{r setup_and_data_visualization_preferences}
library(tidyverse)
library(patchwork)
library(stringr)
library(p8105.datasets)
library(mgcv)
library(modelr)
library(broom)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Simulate data

```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = 
  sim_df_const %>% 
  mutate(
    error = error * 0.75 * x,
    y = 2 + 3 * x + error
  )
```


Plot datasets

```{r}
sim_df_const %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")

sim_df_nonconst %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```

Can see that the second line fits less well b/c error terms are not constant. The error terms in second model vary as a result of X (the predictor variable)

Assumption for linear regression won't fit for sim_df_nonconst. Can still fit OLS regression, but its hard to know how wide confidence interval should be when look at these estimates.

Fit LM for each of these as if OLS assumptions are true. 

```{r}
lm(y ~ x, data = sim_df_const) %>% broom::tidy()
lm(y ~ x, data = sim_df_nonconst) %>% broom::tidy()
```

## Use bootstrap

We know usual OLS assumptions aren't met in this dataset. So let's understand distribution of intercept and slope of repeated samples of datasets. Problem isn't the estimate, it's the confidence intervals.

sample frac draws a sample from df, default is drawing sample as same size as original df.
want sample to be same size b/c CI and variance estimates are very dependent on sample size. So need to keep sample size fixed in bootstrapping.

Draw sample size with replacement so don't get exactly the same sample each time. Mimics collecting new data!

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE) %>% 
    arrange(x)
  
}
```


Check function

```{r}
boot_sample(sim_df_nonconst)
```

Each time you run function, get different sample. It'll draw some values from original sample multiple times, some none at all. Assumes original sample is sufficiently large to be representative of population. 

```{r}
boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  ylim(-5, 16)
```

Line moves every time run it!

Can we estimate slope and intercept and save across bootstrapped samples.

```{r}
boot_sample(sim_df_nonconst) %>% 
  lm(y ~ x, data = .) %>% 
  broom::tidy()
```

Every time we run this, will be slightly different.

## Many samples and analyses

```{r}
boot_straps = 
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
  )

boot_straps %>% pull(strap_sample) %>% .[[1]]
```


Now can use same iterative analyses we used in other places!

Fit regression to each boot strapped df and get some results

```{r}
boot_results =
  boot_straps %>%
  mutate(
    models = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)
  
```

This df has 1000 datasets, and for each dataset, fit linear regression and got intercept and slope for each.

For each dataset, we don't care about std.error, statistic, p.value.

We care about distribution of estimated intercept and slope. We *hope* that these distributions are similar to the standard error.

What is actual std deviation of estimates under repeated sampling?

```{r}
boot_results %>% 
  group_by(term) %>%
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )
``` 

We know linear regression will assume constant variance, and we know constant variance isn't the right assumption....

So, we want to know what the *actual* uncertainty (standard error) is in the intercept and parameter. Not assuming constant (residual) variance here, looking for actual uncertainty in intercept and slope.

If you compare bootstrap sd of intercept

## Confidence intervals

First, look at distributions of bootstrapped results

```{r}
boot_results %>% 
  filter(term == "x") %>% 
  ggplot(aes(x = estimate)) + 
  geom_density()
```

HEY THIS IS COOL! Under repeated sampling, the actual distribution of the slope estimate in my sample does look like a normal distribution. This doesn't rely on any OLS assumptions.

Construct bootstrap CI

```{r}
boot_results %>%
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  )
```


## Bootstrap using modelr

Can we simplify anything...?

```{r}
sim_df_nonconst %>% 
  modelr::bootstrap(1000)
```
 
The modelr package bootstrap function is just like manual function I wrote above.

mgcv::gam package doesn't work well with this

```{r}
## Map a linear model for each of 1000 bootstrap samples
sim_df_nonconst %>% 
  modelr::bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

## Calculate the mean of the bootstrap sample lm parameters
sim_df_nonconst %>% 
  modelr::bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )

```

We were able to get this pretty quickly! This is the actual mean and sd of the parameter estimate on parameter samples. The estimates and standard errors are also pretty close to what they should be if OLS fits.

## Revisit real data

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood
  ) %>% 
  filter(borough != "Staten Island") %>% 
  select(price, stars, borough, neighborhood, room_type)
```

Look at plot 

```{r}
nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price)) + 
  geom_point()
```

THere is a relationship between stars and price, but... SO many outliers, there's definitely non-constant variance. Clearly OLS assumptions don't fit.

```{r}
airbnb_boot_results = 
  nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>% 
  modelr::bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(price ~ stars, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

airbnb_boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )
```


Compare this output to `lm`

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>% 
  lm(price ~ stars, data = .) %>% 
  broom::tidy()
```

LM assuming constant variance thinks the std.error of stars parameter is 4.7, but the bootstrapped actual std.error estimate of stars is 6.4, which is what would expect with outliers that mean assumption of constant variance is not true.

```{r}
airbnb_boot_results %>% 
  filter(term == "stars") %>% 
  ggplot(aes(x = estimate)) + 
  geom_density()
```

Hey, the estimates are not normal distribution! Unlike above. This is real data. So bootstrapped model is really important here.

Transformation on outcome is harder to interpret.
